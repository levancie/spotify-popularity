{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb5164",
   "metadata": {},
   "source": [
    "# Modeling |  Modeling Spotify track popularity\n",
    "## Leo Evancie, Springboard Data Science Career Track\n",
    "\n",
    "This is the fourth step in a capstone project to model music popularity on Spotify, a popular streaming service. Further project details and rationale can be found in the document 'Proposal.pdf'.\n",
    "\n",
    "In this notebook, I will apply my cleaned and processed data to a number of models. For each type of model, I will perform hyperparameter tuning with cross-validation. Ultimately, I will determine which model performs best in predicting whether a given track is popular on Spotify.\n",
    "\n",
    "First, I will read in the data, already split into train and test chunks from the preprocessing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d9d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa50f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "y_train = pd.read_csv('../data/y_train.csv', index_col=0)\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "y_test = pd.read_csv('../data/y_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b91bee",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forests are a popular ensemble model, where several decision trees are trained on bootstrapped samples from the training data. Classification is determined by a survey of decisions from the resulting \"forest\" of individual trees. In order to set a baseline, I will first create a random forest with all default parameters (setting the random state for reproducability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b0ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for baseline RandomForestClassifier: 0.8000676589986468\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, np.ravel(y_train))\n",
    "y_pred = rfc.predict(X_test)\n",
    "rfc_baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score for baseline RandomForestClassifier:', rfc_baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5def8",
   "metadata": {},
   "source": [
    "Now I will tune hyperparameters. The most straightforward method of hyperparameter tuning is grid search with cross-validation. We provide a set of values for any or all model parameters, and the GridSearchCV object systematically tests every possible combination of those given parameters (i.e., it steps through a multidimensional 'grid' of parameter values).\n",
    "\n",
    "The 'CV' in GridSearchCV stands for cross-validation, which is a useful method to stave off overfitting. GridSearchCV applies cross-validation at each step of the grid search. The training data is split into k \"folds\", or equally-sized chunks (the default value for k is 3 in the case of GridSearchCV, but it can be altered). A model is trained from the current set of parameters, and then the model is fit to k-1 folds, leaving the kth aside to act as a test set. A model score is produced and saved. Then, a new model is fit to a different set of k-1 folds (using the same current set of parameters from the grid search), using a different chunk as the test set, producing a second score. This process is performed a total of k times, each iteration using a different chunk as the holdout set, until we have k scores. Those scores are then averaged. This provides us with a score corresponding to a certain set of parameters, and we can be more confident in this score, because the cross-validation lowers the chances that our score was impacted too much by the incidental nature of our overall train/test split. We have, in a sense, simulated the train-test split k times, without ever exposing the model to the real test data.\n",
    "\n",
    "When building a model with relatively few important parameters, and/or when you have reason to only test a small number of values for your parameters, GridSearchCV is feasible. However, as the number and cardinality of your tested parameters increases, grid searching becomes computationally infeasible. This is when we turn to a related method: RandomizedSearchCV. Rather than testing every possible combination of parameter values, RandomSearchCV selects a specified number of combinations, chosen at random, which can drastically reduce the number of models evaluated. And yet this process has been shown to perform at least nearly as well as the brute-force grid search. This is, in part, due to the fact that only a subset of parameters are likely to produce much difference in a model's performance on any given dataset, so to test every possible combination would mean producing a lot of practically redundant models. The random search does a pretty good job capturing, or approximating, the actual variation in performance for the provided parameter grid.\n",
    "\n",
    "Since the RandomForestClassifier has so many potentially impactful parameters, instead of testing every possible combination with a grid search, I will employ the more cost-effective RandomSearchCV. Guidance for this section came from sklearn documentation and this blog post: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9803ad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 800,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,500,800,1000,1500,2000],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [20,50,70,100,None],\n",
    "    'max_features': ['auto','sqrt'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=60,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rfc_random.fit(X_train, np.ravel(y_train))\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3b59a",
   "metadata": {},
   "source": [
    "Even the more time-friendly random search process consumed several minutes. Above, we can see the parameters corresponding to the best-performing model. Now, we can use the search object's `.predict()` method directly, rathern than having to create a new model with the resulting best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6117cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for tuned RandomForestClassifier: 0.8051420838971584\n",
      "Accuracy gained from hyperparameter tuning: 0.005074424898511509\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfc_random.predict(X_test)\n",
    "rfc_tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy score for tuned RandomForestClassifier:', rfc_tuned_accuracy)\n",
    "\n",
    "print('Accuracy gained from hyperparameter tuning:', rfc_tuned_accuracy-rfc_baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1154332",
   "metadata": {},
   "source": [
    "All of that for an additional half-percent of accuracy! Let's look at some of the other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c541b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8051420838971584\n",
      "Recall: 0.8286109191430546\n",
      "Precision: 0.785199738048461\n",
      "F1: 0.8063214525891056\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', rfc_tuned_accuracy)\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ec7a75",
   "metadata": {},
   "source": [
    "Overall, we do see an increase in model performance, but it's certainly not high enough that we shouldn't consider further candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939d7698",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Baseline model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "153b50da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7442489851150202\n",
      "Recall: 0.6993780234968902\n",
      "Precision: 0.7591897974493623\n",
      "F1: 0.7280575539568346\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, np.ravel(y_train))\n",
    "y_pred = lr.predict(X_test)\n",
    "lr_baseline_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', lr_baseline_accuracy)\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb87a72",
   "metadata": {},
   "source": [
    "Next, use simple grid search to test a range of regularization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ba3a7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'random_state': 42}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100,],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_grid = GridSearchCV(estimator=lr, param_grid=param_grid)\n",
    "lr_grid.fit(X_train, np.ravel(y_train))\n",
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e5b6a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7439106901217862\n",
      "Recall: 0.7042156185210781\n",
      "Precision: 0.7559347181008902\n",
      "F1: 0.7291592128801431\n",
      "Accuracy gained from tuning: -0.0003382949932340118\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_grid.predict(X_test)\n",
    "lr_tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', lr_tuned_accuracy)\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test,y_pred))\n",
    "print('Accuracy gained from tuning:', lr_tuned_accuracy-lr_baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad357be",
   "metadata": {},
   "source": [
    "Here's an interesting result. LogisticRegression's default value for C is 1. GridSearchCV tested a range of C values, including 1, and judged 10 to be a better value. But here, we see that C=10 actually produced very slightly lower accuracy. From this, we can deduce that accuracy is not the default measure that GridSearchCV uses to determine best parameters.\n",
    "\n",
    "Looking at the other metrics, we can see that the tuned model yielded lower accuracy and precision, but higher recall and f1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28327571",
   "metadata": {},
   "source": [
    "### K-nearest neighbors\n",
    "\n",
    "Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81175c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7642083897158322\n",
      "Recall: 0.8113337940566689\n",
      "Precision: 0.7346683354192741\n",
      "F1: 0.7711001642036126\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, np.ravel(y_train))\n",
    "y_pred = knn.predict(X_test)\n",
    "knn_baseline_f1 = f1_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1:', knn_baseline_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642d94c",
   "metadata": {},
   "source": [
    "Next, perform a grid search over a range of number of neighbors, as well as uniform vs. distance-based weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86540a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9, 'weights': 'distance'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,3,5,7,9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid)\n",
    "knn_grid.fit(X_train, np.ravel(y_train))\n",
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25cabfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7713125845737483\n",
      "Recall: 0.816171389080857\n",
      "Precision: 0.7423004399748586\n",
      "F1: 0.7774851876234364\n",
      "F1 gained from tuning: 0.006385023419823832\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_grid.predict(X_test)\n",
    "knn_tuned_f1 = f1_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('F1:', knn_tuned_f1)\n",
    "print('F1 gained from tuning:', knn_tuned_f1-knn_baseline_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556aa10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
