{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb5164",
   "metadata": {},
   "source": [
    "# Modeling |  Modeling Spotify track popularity\n",
    "## Leo Evancie, Springboard Data Science Career Track\n",
    "\n",
    "This is the fourth step in a capstone project to model music popularity on Spotify, a popular streaming service. Further project details and rationale can be found in the document 'Proposal.pdf'.\n",
    "\n",
    "In this notebook, I will apply my cleaned and processed data to a number of models. For each type of model, I will perform hyperparameter tuning with cross-validation. Ultimately, I will determine which model performs best in predicting whether a given track is popular on Spotify.\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. [Load libraries and data](#load)\n",
    "2. [Logistic regression](#logistic)\n",
    "3. [K-nearest neighbors](#knn)\n",
    "4. [Random forest](#rf)\n",
    "5. [Gradient boosting](#gb)\n",
    "6. [Comparing model performance](#compare)\n",
    "7. [Conclusions and next steps](#conclusion)\n",
    "\n",
    "## 1. Load libraries and data <a class=\"anchor\" id=\"load\"></a>\n",
    "\n",
    "First, I will read in the data, already split into train and test chunks from the preprocessing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d9d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa50f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "y_train = np.ravel(pd.read_csv('../data/y_train.csv', index_col=0))\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "y_test = np.ravel(pd.read_csv('../data/y_test.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b543dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6896 entries, 9671 to 2191\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   single            6896 non-null   int64  \n",
      " 1   danceability      6896 non-null   float64\n",
      " 2   energy            6896 non-null   float64\n",
      " 3   instrumentalness  6896 non-null   float64\n",
      " 4   explicit          6896 non-null   int64  \n",
      " 5   collab            6896 non-null   int64  \n",
      " 6   timesig_0         6896 non-null   int64  \n",
      " 7   timesig_1         6896 non-null   int64  \n",
      " 8   timesig_3         6896 non-null   int64  \n",
      " 9   timesig_4         6896 non-null   int64  \n",
      " 10  timesig_5         6896 non-null   int64  \n",
      " 11  track_number      6896 non-null   float64\n",
      " 12  duration_s        6896 non-null   float64\n",
      "dtypes: float64(5), int64(8)\n",
      "memory usage: 754.2 KB\n"
     ]
    }
   ],
   "source": [
    "# Reminder of the training data features\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644d986",
   "metadata": {},
   "source": [
    "## 2. Logistic regression <a class=\"anchor\" id=\"logistic\"></a>\n",
    "\n",
    "A standard classification model, I will run a baseline logistic regression first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b444ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LR classification report (training):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      3558\n",
      "           1       0.76      0.83      0.79      3338\n",
      "\n",
      "    accuracy                           0.79      6896\n",
      "   macro avg       0.79      0.79      0.79      6896\n",
      "weighted avg       0.80      0.79      0.79      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "# Generate predictions from training data for training scores\n",
    "y_train_pred = lr.predict(X_train)\n",
    "# Save baseline f1 training score for comparison to testing & tuned scores\n",
    "lr_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline LR classification report (training):\\n')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8599649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basline LR classification report (testing):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      1509\n",
      "           1       0.76      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.79      2956\n",
      "   macro avg       0.80      0.79      0.79      2956\n",
      "weighted avg       0.80      0.79      0.79      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from testing data for testing scores\n",
    "y_test_pred = lr.predict(X_test)\n",
    "lr_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Basline LR classification report (testing):\\n')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dd805",
   "metadata": {},
   "source": [
    "With a simple baseline logistic regression model, I see an f1 test score of 0.80. With similar scores for training and testing, we do not see evidence of overfit.\n",
    "\n",
    "Now, I will tune hyperparameters. The most straightforward method of hyperparameter tuning is grid search with cross-validation. We provide a set of values for any or all model parameters, and the GridSearchCV object systematically tests every possible combination of those given parameters (i.e., it steps through a multidimensional 'grid' of parameter values).\n",
    "\n",
    "The 'CV' in GridSearchCV stands for cross-validation, which is a useful method to stave off overfitting. GridSearchCV applies cross-validation at each step of the grid search. The training data is split into k \"folds\", or equally-sized chunks (the default value for k is 3 in the case of GridSearchCV, but it can be altered). A model is trained from the current set of parameters, and then the model is fit to k-1 folds, leaving the kth aside to act as a test set. A model score is produced and saved. Then, a new model is fit to a different set of k-1 folds (using the same current set of parameters from the grid search), using a different chunk as the test set, producing a second score. This process is performed a total of k times, each iteration using a different chunk as the holdout set, until we have k scores. Those scores are then averaged. This provides us with a score corresponding to a certain set of parameters, and we can be more confident in this score, because the cross-validation lowers the chances that our score was impacted too much by the incidental nature of our overall train/test split. We have, in a sense, simulated the train-test split k times, without ever exposing the model to the real test data.\n",
    "\n",
    "Here, I will only test a range of regularization parameters, along with a random state for the sake of reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4fd6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'random_state': 42}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100,],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_grid = GridSearchCV(estimator=lr, param_grid=param_grid)\n",
    "start_time = time()\n",
    "lr_grid.fit(X_train, y_train)\n",
    "# Capture wall time for fitting the tuned model for later comparison\n",
    "lr_grid_fit_time = time() - start_time\n",
    "# Display best parameters from grid search\n",
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d12675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LR classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      3558\n",
      "           1       0.76      0.83      0.79      3338\n",
      "\n",
      "    accuracy                           0.79      6896\n",
      "   macro avg       0.79      0.79      0.79      6896\n",
      "weighted avg       0.79      0.79      0.79      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr_grid.predict(X_train)\n",
    "lr_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned LR classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042bcff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LR classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      1509\n",
      "           1       0.76      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.79      2956\n",
      "   macro avg       0.80      0.80      0.79      2956\n",
      "weighted avg       0.80      0.79      0.79      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = lr_grid.predict(X_test)\n",
    "# Capture wall time for making predictions for later comparison\n",
    "lr_grid_pred_time = time() - start_time\n",
    "lr_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned LR classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b26f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in LR f1 score from parameter tuning: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Change in LR f1 score from parameter tuning:', round(lr_tuned_f1_test-lr_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001c642",
   "metadata": {},
   "source": [
    "There was barely any improvement, which is not entirely unexpected given the fact that I only tested a small number of values for one single parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4188c",
   "metadata": {},
   "source": [
    "## 3. K-nearest neighbors <a class=\"anchor\" id=\"knn\"></a>\n",
    "\n",
    "The k-nearest neighbors model creates k clusters of points in your dataset where each cluster is assumed to share a class. The number of clusters is set by the user, and the positions and sizes of the clusters are determined mathematically. Here's a baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114c432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline KNN classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84      3558\n",
      "           1       0.81      0.89      0.85      3338\n",
      "\n",
      "    accuracy                           0.85      6896\n",
      "   macro avg       0.85      0.85      0.85      6896\n",
      "weighted avg       0.85      0.85      0.85      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "knn_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline KNN classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef99034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline KNN classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.77      1509\n",
      "           1       0.75      0.83      0.79      1447\n",
      "\n",
      "    accuracy                           0.78      2956\n",
      "   macro avg       0.78      0.78      0.78      2956\n",
      "weighted avg       0.78      0.78      0.78      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = knn.predict(X_test)\n",
    "knn_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline KNN classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf99a01",
   "metadata": {},
   "source": [
    "The baseline KNN model displayed some overfitting.\n",
    "\n",
    "Next, I will tune parameters by performing a grid search over a range of numbers of neighbors, as well as uniform vs. distance-based weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993a6b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 7, 'weights': 'distance'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,3,5,7,9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid)\n",
    "start_time = time()\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_grid_fit_time = time() - start_time\n",
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c941743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3558\n",
      "           1       1.00      1.00      1.00      3338\n",
      "\n",
      "    accuracy                           1.00      6896\n",
      "   macro avg       1.00      1.00      1.00      6896\n",
      "weighted avg       1.00      1.00      1.00      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = knn_grid.predict(X_train)\n",
    "knn_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned KNN classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94410e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78      1509\n",
      "           1       0.75      0.83      0.79      1447\n",
      "\n",
      "    accuracy                           0.78      2956\n",
      "   macro avg       0.79      0.78      0.78      2956\n",
      "weighted avg       0.79      0.78      0.78      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = knn_grid.predict(X_test)\n",
    "knn_grid_pred_time = time() - start_time\n",
    "knn_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned KNN classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e83041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(knn_tuned_f1_test-knn_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95756e",
   "metadata": {},
   "source": [
    "The model has memorized our training data, producing 100% training scores. And with an f1 test score of 0.79, the tuned KNN performed a little worse than the tuned LR model.\n",
    "\n",
    "Logistic regression and k-nearest neighbors are each single models, whereby classification is determined by a single calculation, and that calculation is shaped by the parameters the model learns from the training data. Such models can perform quite well under many circumstances.\n",
    "\n",
    "But we also have \"ensemble models\", which produce a large group of similarly-structured models, all of which get a say in the final classification decision. Ensemble models are shown to be more robust, because while a given individual model in the ensemble may produce a bias in a certain direction, another model will likely produce bias of a similar magnitude in the opposite direction. With enough members in the ensemble, the bias gets smoothed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b91bee",
   "metadata": {},
   "source": [
    "## 4. Random Forest <a class=\"anchor\" id=\"rf\"></a>\n",
    "\n",
    "Random forests are a popular ensemble model, where several decision trees are trained on bootstrapped samples from the training data. Classification is determined by a survey of decisions from the resulting \"forest\" of individual trees. In order to set a baseline, I will first create a random forest with all default parameters (setting the random state for reproducability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0808bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RFC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3558\n",
      "           1       1.00      1.00      1.00      3338\n",
      "\n",
      "    accuracy                           1.00      6896\n",
      "   macro avg       1.00      1.00      1.00      6896\n",
      "weighted avg       1.00      1.00      1.00      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_train_pred = rfc.predict(X_train)\n",
    "rfc_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline RFC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26b0ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RFC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83      1509\n",
      "           1       0.81      0.85      0.83      1447\n",
      "\n",
      "    accuracy                           0.83      2956\n",
      "   macro avg       0.83      0.83      0.83      2956\n",
      "weighted avg       0.83      0.83      0.83      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = rfc.predict(X_test)\n",
    "rfc_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline RFC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564b914",
   "metadata": {},
   "source": [
    "While we still see some overfitting to the training data, even the out-of-the-box random forest performs better on the test data than the tuned versions of our previous model candidates. Let's see what happens after tuning.\n",
    "\n",
    "When building a model with relatively few important parameters, and/or when you have reason to only test a small number of values for your parameters, GridSearchCV is feasible. However, as the number of parameters and values-per-parameter increase, grid searching quickly becomes infeasible in terms of computing capacity and time. This is when we turn to a related method: RandomizedSearchCV. Rather than testing every possible combination of parameter values, RandomSearchCV selects a specified number of combinations, chosen at random, which drastically reduces the number of models evaluated. And yet this process has been shown to perform at least nearly as well as the brute-force grid search. This is likely due to the fact that only a subset of parameters are likely to produce much difference in a model's performance on any given dataset. To test every possible combination would mean producing a lot of practically redundant models. The randomized search does a pretty good job capturing, or approximating, the actual variation in performance for the provided parameter grid.\n",
    "\n",
    "Since the RandomForestClassifier has so many potentially impactful parameters, I will employ RandomSearchCV. Guidance for this section came from sklearn documentation and this blog post: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74. Since there are about 1,000 parameter combinations in my grid, I will test `n_iter = 100` combinations, or 10% of the grid space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9803ad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 2000,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 100,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,500,800,1000,1500,2000],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [20,50,70,100,None],\n",
    "    'max_features': ['auto','sqrt'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "rfc_random.fit(X_train, y_train)\n",
    "rfc_random_fit_time = time() - start_time\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc0611",
   "metadata": {},
   "source": [
    "The random search process took considerable time. I would not even attempt a full GridSearchCV with the above parameter grid. Let's see how the tuned model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c938575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned RFC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95      3558\n",
      "           1       0.93      0.96      0.95      3338\n",
      "\n",
      "    accuracy                           0.95      6896\n",
      "   macro avg       0.95      0.95      0.95      6896\n",
      "weighted avg       0.95      0.95      0.95      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = rfc_random.predict(X_train)\n",
    "rfc_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned RFC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f8b7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned RFC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.82      1509\n",
      "           1       0.80      0.85      0.83      1447\n",
      "\n",
      "    accuracy                           0.83      2956\n",
      "   macro avg       0.83      0.83      0.83      2956\n",
      "weighted avg       0.83      0.83      0.83      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = rfc_random.predict(X_test)\n",
    "rfc_random_pred_time = time() - start_time\n",
    "rfc_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned RFC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6117cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(rfc_tuned_f1_test-rfc_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fef6a",
   "metadata": {},
   "source": [
    "While the random forest model showed the best baseline metrics so far, we see barely any improvement from parameter tuning. We see less overfitting in the tuned model, though, with the training scores around 95% instead of 100%. \n",
    "\n",
    "But is this the best we can do?\n",
    "\n",
    "## 5. Gradient boosting <a class=\"anchor\" id=\"gb\"></a>\n",
    "\n",
    "The process of training a random forest classifier involves creating any number of _independent_ decision trees. The gradient boosting classifier, also a tree-based ensemble model, involves training a number of trees _in sequence_, where each subsequent tree is fitted to the error of the previous tree. In this way, each tree learns more about the data, getting better in specific ways to compensate for past shortcomings. Ultimate classification is determined by the final \"smartest\" tree in the sequence. Let's see how a baseline GB model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9db1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GBC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      3558\n",
      "           1       0.81      0.88      0.84      3338\n",
      "\n",
      "    accuracy                           0.84      6896\n",
      "   macro avg       0.84      0.84      0.84      6896\n",
      "weighted avg       0.84      0.84      0.84      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "y_train_pred = gbc.predict(X_train)\n",
    "gbc_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline GBC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8602b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GBC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.82      1509\n",
      "           1       0.79      0.86      0.82      1447\n",
      "\n",
      "    accuracy                           0.82      2956\n",
      "   macro avg       0.82      0.82      0.82      2956\n",
      "weighted avg       0.82      0.82      0.82      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = gbc.predict(X_test)\n",
    "gbc_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline GBC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9337fb",
   "metadata": {},
   "source": [
    "The testing scores of the RFC and GBC were quite simliar. However, we see _much_ less overfitting in the GBC model. This is not surprising, since the GBC model is designed to reduce overfitting. Hopefully, we can improve performance with tuning, so that we have the sweet-spot of good testing scores and no overfitting.\n",
    "\n",
    "I will again employ the randomized search with cross-validation. This grid could produce about 5,000 parameter combinations, so I will use a higher `n_iter` value to still capture a good portion of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "328c47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 50,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 5,\n",
       " 'loss': 'deviance',\n",
       " 'learning_rate': 0.1,\n",
       " 'criterion': 'friedman_mse'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'learning_rate': [0.01,0.1,1],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'criterion': ['friedman_mse','mse'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'max_depth': [2,3,4,5],\n",
    "    'random_state': [42],\n",
    "    'max_features': ['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_random = RandomizedSearchCV(\n",
    "    estimator=gbc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=500,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "gbc_random.fit(X_train, y_train)\n",
    "gbc_random_fit_time = time() - start_time\n",
    "gbc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0deb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned GBC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86      3558\n",
      "           1       0.83      0.89      0.86      3338\n",
      "\n",
      "    accuracy                           0.86      6896\n",
      "   macro avg       0.86      0.86      0.86      6896\n",
      "weighted avg       0.86      0.86      0.86      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = gbc_random.predict(X_train)\n",
    "gbc_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned GBC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9698139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned GBC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81      1509\n",
      "           1       0.79      0.85      0.82      1447\n",
      "\n",
      "    accuracy                           0.82      2956\n",
      "   macro avg       0.82      0.82      0.82      2956\n",
      "weighted avg       0.82      0.82      0.82      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = gbc_random.predict(X_test)\n",
    "gbc_random_pred_time = time() - start_time\n",
    "gbc_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned GBC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0345a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: -0.003\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(gbc_tuned_f1_test-gbc_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64afa561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8216539355695782, 0.8244628099173554)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_tuned_f1_test, gbc_baseline_f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834bba9",
   "metadata": {},
   "source": [
    "The GBC was much more resistant to overfitting to the training data. Curiously, parameter tuning led to a slight decrease in f1 score.\n",
    "\n",
    "## 6. Comparing model performance <a class=\"anchor\" id=\"compare\"></a>\n",
    "\n",
    "Throughout the previous section, I checked the effect of tuning on a key model metric. I chose the f1 score as the key metric, because it is a good balance (i.e., harmonic mean) of precision and recall scores. In this use case, there is no reason to think we need to give special priority to reducing either false-positive or false-negative scores, so we do not particularly need to optimize precision or recall.\n",
    "\n",
    "Now, I will organize f1 scores (basline and tuned, training and testing) for each model, as well as fit and prediction times, into a DataFrame for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38677109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_f1_train</th>\n",
       "      <th>baseline_f1_test</th>\n",
       "      <th>tuned_f1_train</th>\n",
       "      <th>tuned_f1_test</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.794721</td>\n",
       "      <td>0.799737</td>\n",
       "      <td>0.794320</td>\n",
       "      <td>0.800920</td>\n",
       "      <td>1.213788</td>\n",
       "      <td>0.004575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.847530</td>\n",
       "      <td>0.786702</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.788936</td>\n",
       "      <td>7.004531</td>\n",
       "      <td>0.259440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC</th>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.826586</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.827748</td>\n",
       "      <td>3944.163748</td>\n",
       "      <td>1.931742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.842378</td>\n",
       "      <td>0.824463</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.821654</td>\n",
       "      <td>1796.987758</td>\n",
       "      <td>0.016631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        baseline_f1_train  baseline_f1_test  tuned_f1_train  tuned_f1_test  \\\n",
       "LogReg           0.794721          0.799737        0.794320       0.800920   \n",
       "KNN              0.847530          0.786702        0.999850       0.788936   \n",
       "RFC              0.999850          0.826586        0.945139       0.827748   \n",
       "GBC              0.842378          0.824463        0.856274       0.821654   \n",
       "\n",
       "           fit_time  prediction_time  \n",
       "LogReg     1.213788         0.004575  \n",
       "KNN        7.004531         0.259440  \n",
       "RFC     3944.163748         1.931742  \n",
       "GBC     1796.987758         0.016631  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        'baseline_f1_train':[lr_baseline_f1_train, knn_baseline_f1_train, rfc_baseline_f1_train, gbc_baseline_f1_train],\n",
    "        'baseline_f1_test':[lr_baseline_f1_test, knn_baseline_f1_test, rfc_baseline_f1_test, gbc_baseline_f1_test],\n",
    "        'tuned_f1_train':[lr_tuned_f1_train, knn_tuned_f1_train, rfc_tuned_f1_train, gbc_tuned_f1_train],\n",
    "        'tuned_f1_test':[lr_tuned_f1_test, knn_tuned_f1_test, rfc_tuned_f1_test, gbc_tuned_f1_test],\n",
    "        'fit_time':[lr_grid_fit_time, knn_grid_fit_time, rfc_random_fit_time, gbc_random_fit_time],\n",
    "        'prediction_time':[lr_grid_pred_time, knn_grid_pred_time, rfc_random_pred_time, gbc_random_pred_time]\n",
    "    },\n",
    "    index=['LogReg','KNN','RFC','GBC']\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e03205",
   "metadata": {},
   "source": [
    "In every case except for the GBC, parameter-tuning yielded some improvement in f1, both in terms of training and test scores. Although the tuned RFC showed the highest test f1 score (by an ever-so-slight margin), we see a greater degree of over-fitting via the training score as compared to the GBC. Furthermore, compared to the RFC, the GBC required about 1/3 of the wall-time to fit the data, and about 1/9 of the time to generate predictions.\n",
    "\n",
    "## 7. Conclusions and next steps <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "After collecting ten thousand tracks from Spotify's API, performing exploratory data analysis, cleaning and preprocessing the data, and eliminating what appeared to be irrelevant/redundant features, I ended up with a set of 9,852 tracks, each containing the following features:\n",
    "\n",
    "- `single` - (Bool) Standalone single or part of an album/compilation\n",
    "- `danceability`, `energy`, `instrumentalness` - (Float) Spotify-provided measurements of musical qualities, scaled 0 to 1\n",
    "- `explicit` - (Bool) Includes explicit lyrics\n",
    "- `collab` - (Bool) Includes one or more guest artists\n",
    "- `timesig` - (Category) Time signature; a rhythmic measurement\n",
    "- `duration_s` - (Float) Length in seconds, scaled 0 to 1\n",
    "- `track_number` - (Float) Position of track on corresponding album, scaled 0 to 1\n",
    "- __`popularity`__ (Bool) Target\n",
    "\n",
    "I tried two different classification models: Logistic regression and k-nearest neighbors. Then, I tried two ensemble classification models: Random forest and gradient boosting. For all four model types, I applied some hyperparameter-tuning in an attempt to optimize performance.\n",
    "\n",
    "In addition to viewing classification reports for all models, I specifically recorded the train f1 and test f1 scores, singling these out as the primary metrics for comparison. I chose the f1 score due to its nature as a harmonic mean of precision and recall; there is no apparent reason to optimize either individually in this use-case.  I also recorded wall-times for the fitting and predicting steps of each model, for a gauge of practicality in deployment.\n",
    "\n",
    "The random forest classifier yielded a test f1 score of 80.6%, to the gradient boosting classifier's 80.3% This 0.03% difference is not trivial, especially given the big-data context of Spotify's library and user base. However, the RFC's train f1 score is 95.1% to the GBC's 86.7%, indicating much more over-fitting in the RFC. This means that we can have somewhat less confidence in the RFC's ability to generalize to new data (beyond the test set), especially with such a slim margin between the test f1 scores. Moreover, the fitting and predicting times for the RFC were drastically higher than the GBC, and that is an important practical consideration when deploying an ML model.\n",
    "\n",
    "__Overall, I judge the tuned gradient boosting classifier to be the superior model among these candidates__. With further time, one could dig deeper into tuning the random forest, perhaps with Bayesian optimization, in the hopes of bringing that test f1 score closer to the train f1 score. If the RFC's f1 score could be improved sufficiently higher than that of the GBC, that could outweigh its lengthy fit and predict times and make it the superior model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c8b3706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = open(\"../model_summary.txt\",\"w\")\n",
    "\n",
    "model_features = [\n",
    "    'Features:\\n\\n',\n",
    "    'single - (Bool) Standalone single or part of an album/compilation \\n',\n",
    "    'danceability - (Float) Spotify-provided measurement of musical quality, scaled 0 to 1 \\n',\n",
    "    'energy - see above \\n',\n",
    "    'instrumentalness - see above \\n',\n",
    "    'explicit - (Bool) Includes explicit lyrics \\n',\n",
    "    'collab - (Bool) Includes one or more guest artists \\n',\n",
    "    'timesig - (Category) Time signature; a rhythmic measurement \\n',\n",
    "    'duration_s - (Float) Length in seconds, scaled 0 to 1 \\n',\n",
    "    'track_number - (Float) Track number, max 25, scaled 0 to 1 \\n',\n",
    "    'popularity - (Bool, TARGET) 1 if raw popularity score > 50, else 0 \\n\\n\\n']\n",
    "metrics.writelines(model_features)\n",
    "\n",
    "metrics.write('Parameters: \\n\\n')\n",
    "model_params = str(gbc_random.best_estimator_).split()\n",
    "model_params = [x+' \\n' for x in model_params]\n",
    "metrics.writelines(model_params)\n",
    "\n",
    "metrics.write('\\n\\n')\n",
    "metrics.write('F1 test score: '+ str(round(gbc_tuned_f1_test,3)))\n",
    "\n",
    "metrics.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8f5db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
