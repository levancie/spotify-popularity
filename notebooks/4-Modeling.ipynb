{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb5164",
   "metadata": {},
   "source": [
    "# Modeling |  Modeling Spotify track popularity\n",
    "## Leo Evancie, Springboard Data Science Career Track\n",
    "\n",
    "This is the fourth step in a capstone project to model music popularity on Spotify, a popular streaming service. Further project details and rationale can be found in the document 'Proposal.pdf'.\n",
    "\n",
    "In this notebook, I will apply my cleaned and processed data to a number of models. For each type of model, I will perform hyperparameter tuning with cross-validation. Ultimately, I will determine which model performs best in predicting whether a given track is popular on Spotify.\n",
    "\n",
    "First, I will read in the data, already split into train and test chunks from the preprocessing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d9d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa50f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "y_train = np.ravel(pd.read_csv('../data/y_train.csv', index_col=0))\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "y_test = np.ravel(pd.read_csv('../data/y_test.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644d986",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Baseline model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b444ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76      1509\n",
      "           1       0.76      0.70      0.73      1447\n",
      "\n",
      "    accuracy                           0.74      2956\n",
      "   macro avg       0.75      0.74      0.74      2956\n",
      "weighted avg       0.75      0.74      0.74      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "#save baseline f1 score for later comparison to tuned model\n",
    "lr_baseline_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dd805",
   "metadata": {},
   "source": [
    "Now I will tune hyperparameters. The most straightforward method of hyperparameter tuning is grid search with cross-validation. We provide a set of values for any or all model parameters, and the GridSearchCV object systematically tests every possible combination of those given parameters (i.e., it steps through a multidimensional 'grid' of parameter values).\n",
    "\n",
    "The 'CV' in GridSearchCV stands for cross-validation, which is a useful method to stave off overfitting. GridSearchCV applies cross-validation at each step of the grid search. The training data is split into k \"folds\", or equally-sized chunks (the default value for k is 3 in the case of GridSearchCV, but it can be altered). A model is trained from the current set of parameters, and then the model is fit to k-1 folds, leaving the kth aside to act as a test set. A model score is produced and saved. Then, a new model is fit to a different set of k-1 folds (using the same current set of parameters from the grid search), using a different chunk as the test set, producing a second score. This process is performed a total of k times, each iteration using a different chunk as the holdout set, until we have k scores. Those scores are then averaged. This provides us with a score corresponding to a certain set of parameters, and we can be more confident in this score, because the cross-validation lowers the chances that our score was impacted too much by the incidental nature of our overall train/test split. We have, in a sense, simulated the train-test split k times, without ever exposing the model to the real test data.\n",
    "\n",
    "Here, I will only test a range of regularization parameters, along with a random state for the sake of reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab4fd6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'random_state': 42}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100,],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_grid = GridSearchCV(estimator=lr, param_grid=param_grid)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d12675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76      1509\n",
      "           1       0.76      0.70      0.73      1447\n",
      "\n",
      "    accuracy                           0.74      2956\n",
      "   macro avg       0.74      0.74      0.74      2956\n",
      "weighted avg       0.74      0.74      0.74      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr_grid.predict(X_test)\n",
    "lr_tuned_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b26f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(lr_tuned_f1-lr_baseline_f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001c642",
   "metadata": {},
   "source": [
    "There was barely any improvement, which is not entirely unexpected given the fact that I only tested a small number of values for one single parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4188c",
   "metadata": {},
   "source": [
    "### K-nearest neighbors\n",
    "\n",
    "Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef99034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      1509\n",
      "           1       0.73      0.81      0.77      1447\n",
      "\n",
      "    accuracy                           0.76      2956\n",
      "   macro avg       0.77      0.77      0.76      2956\n",
      "weighted avg       0.77      0.76      0.76      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "knn_baseline_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf99a01",
   "metadata": {},
   "source": [
    "Next, I perform a grid search over a range of numbers of neighbors, as well as uniform vs. distance-based weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993a6b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9, 'weights': 'distance'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,3,5,7,9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c941743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.76      1509\n",
      "           1       0.74      0.82      0.78      1447\n",
      "\n",
      "    accuracy                           0.77      2956\n",
      "   macro avg       0.77      0.77      0.77      2956\n",
      "weighted avg       0.77      0.77      0.77      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_grid.predict(X_test)\n",
    "knn_tuned_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7e83041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.006\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(knn_tuned_f1-knn_baseline_f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b91bee",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forests are a popular ensemble model, where several decision trees are trained on bootstrapped samples from the training data. Classification is determined by a survey of decisions from the resulting \"forest\" of individual trees. In order to set a baseline, I will first create a random forest with all default parameters (setting the random state for reproducability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26b0ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      1509\n",
      "           1       0.79      0.81      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "rfc_baseline_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564b914",
   "metadata": {},
   "source": [
    "Even the out-of-the-box random forest performs better across the board than the tuned versions of our previous model candidates. Let's see what happens after tuning.\n",
    "\n",
    "When building a model with relatively few important parameters, and/or when you have reason to only test a small number of values for your parameters, GridSearchCV is feasible. However, as the number of parameters and values-per-parameter increase, grid searching quickly becomes infeasible in terms of computing capacity and time. This is when we turn to a related method: RandomizedSearchCV. Rather than testing every possible combination of parameter values, RandomSearchCV selects a specified number of combinations, chosen at random, which drastically reduces the number of models evaluated. And yet this process has been shown to perform at least nearly as well as the brute-force grid search. This is likely due to the fact that only a subset of parameters are likely to produce much difference in a model's performance on any given dataset. To test every possible combination would mean producing a lot of practically redundant models. The randomized search does a pretty good job capturing, or approximating, the actual variation in performance for the provided parameter grid.\n",
    "\n",
    "Since the RandomForestClassifier has so many potentially impactful parameters, I will employ the more cost-effective RandomSearchCV. Guidance for this section came from sklearn documentation and this blog post: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9803ad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 800,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,500,800,1000,1500,2000],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [20,50,70,100,None],\n",
    "    'max_features': ['auto','sqrt'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=60,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rfc_random.fit(X_train, y_train)\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc0611",
   "metadata": {},
   "source": [
    "Even the more time-friendly random search process consumed several minutes. I would not even attempt a GridSearchCV with the above parameter grid. Let's see how the tuned model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c938575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80      1509\n",
      "           1       0.79      0.83      0.81      1447\n",
      "\n",
      "    accuracy                           0.81      2956\n",
      "   macro avg       0.81      0.81      0.81      2956\n",
      "weighted avg       0.81      0.81      0.81      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rfc_random.predict(X_test)\n",
    "rfc_tuned_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6117cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.009\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(rfc_tuned_f1-rfc_baseline_f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fef6a",
   "metadata": {},
   "source": [
    "Nearly a full percentage-point increase in the f1 score. That means that not only did the random forest model have the best baseline metrics, it also responded most drastically to our parameter tuning. This is a good illustration of the power of ensemble models, drawing on the \"wisdom of crowds\" to generate classifications by surveying outputs from hundreds of models.\n",
    "\n",
    "But is this the best we can do?\n",
    "\n",
    "### GradientBoostingClassifier\n",
    "\n",
    "The process of training a random forest classifier involves creating any number of _independent_ decision trees. The gradient boosting classifier, also a tree-based ensemble model, involves training a number of trees _in sequence_, where each subsequent tree is fitted to the error of the previous tree. In this way, each tree learns more about the data, getting better in specific ways to compensate for past shortcomings. Let's see how a baseline GB model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9db1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      1509\n",
      "           1       0.77      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "gbc_baseline_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9337fb",
   "metadata": {},
   "source": [
    "Similar performance to the baseline RF classifier. Now, let's tune. I will again employ the randomized search with cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "328c47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 59s ± 6.96 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 100,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 5,\n",
       " 'loss': 'deviance',\n",
       " 'learning_rate': 0.1,\n",
       " 'criterion': 'friedman_mse'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'learning_rate': [0.01,0.1,1],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'criterion': ['friedman_mse','mse'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'max_depth': [2,3,4,5],\n",
    "    'random_state': [42],\n",
    "    'max_features': ['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_random = RandomizedSearchCV(\n",
    "    estimator=gbc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=60,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "%timeit gbc_random.fit(X_train, y_train)\n",
    "gbc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0deb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      1509\n",
      "           1       0.77      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbc_random.predict(X_test)\n",
    "gbc_tuned_f1 = f1_score(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0345a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.002\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(gbc_tuned_f1-gbc_baseline_f1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc80d3b",
   "metadata": {},
   "source": [
    "So, while the default GB model performed similarly to the default RF model, the GB model was not improved nearly as much by parameter tuning as the RF model was.\n",
    "\n",
    "## Comparing model performance and the effects of parameter tuning\n",
    "\n",
    "Throughout the previous section, I checked the effect of tuning on a key model metric. I chose the f1 score as the key metric, because it is a good balance (i.e., harmonic mean) of precision and recall scores. (In this use case, there is no reason to think we need to give special priority to reducing either false-positive or false-negative scores, so we do not need to particularly optimize precision or recall.)\n",
    "\n",
    "Now, I will organize both the baseline and tuned f1 scores for each model into a DataFrame for easy comparison and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38677109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_f1</th>\n",
       "      <th>tuned_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.728058</td>\n",
       "      <td>0.729159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.771100</td>\n",
       "      <td>0.777485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC</th>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.806321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.800265</td>\n",
       "      <td>0.800265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        baseline_f1  tuned_f1\n",
       "LogReg     0.728058  0.729159\n",
       "KNN        0.771100  0.777485\n",
       "RFC        0.797672  0.806321\n",
       "GBC        0.800265  0.800265"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        'baseline_f1':[lr_baseline_f1, knn_baseline_f1, rfc_baseline_f1, gbc_baseline_f1],\n",
    "        'tuned_f1':[lr_tuned_f1, knn_tuned_f1, rfc_tuned_f1, gbc_baseline_f1]\n",
    "    },\n",
    "    index=['LogReg','KNN','RFC','GBC']\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77434004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
