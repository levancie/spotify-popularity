{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdb5164",
   "metadata": {},
   "source": [
    "# Modeling |  Modeling Spotify track popularity\n",
    "## Leo Evancie, Springboard Data Science Career Track\n",
    "\n",
    "This is the fourth step in a capstone project to model music popularity on Spotify, a popular streaming service. Further project details and rationale can be found in the document 'Proposal.pdf'.\n",
    "\n",
    "In this notebook, I will apply my cleaned and processed data to a number of models. For each type of model, I will perform hyperparameter tuning with cross-validation. Ultimately, I will determine which model performs best in predicting whether a given track is popular on Spotify.\n",
    "\n",
    "First, I will read in the data, already split into train and test chunks from the preprocessing stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d9d1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa50f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../data/X_train.csv', index_col=0)\n",
    "y_train = np.ravel(pd.read_csv('../data/y_train.csv', index_col=0))\n",
    "X_test = pd.read_csv('../data/X_test.csv', index_col=0)\n",
    "y_test = np.ravel(pd.read_csv('../data/y_test.csv', index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b543dc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6896 entries, 9671 to 2191\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   single            6896 non-null   int64  \n",
      " 1   danceability      6896 non-null   float64\n",
      " 2   energy            6896 non-null   float64\n",
      " 3   instrumentalness  6896 non-null   float64\n",
      " 4   explicit          6896 non-null   int64  \n",
      " 5   collab            6896 non-null   int64  \n",
      " 6   timesig_0         6896 non-null   int64  \n",
      " 7   timesig_1         6896 non-null   int64  \n",
      " 8   timesig_3         6896 non-null   int64  \n",
      " 9   timesig_4         6896 non-null   int64  \n",
      " 10  timesig_5         6896 non-null   int64  \n",
      " 11  duration_s        6896 non-null   float64\n",
      "dtypes: float64(4), int64(8)\n",
      "memory usage: 700.4 KB\n"
     ]
    }
   ],
   "source": [
    "# Reminder of the training data features\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644d986",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Baseline model first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b444ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline LR classification report (training):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76      3558\n",
      "           1       0.75      0.69      0.72      3338\n",
      "\n",
      "    accuracy                           0.74      6896\n",
      "   macro avg       0.74      0.74      0.74      6896\n",
      "weighted avg       0.74      0.74      0.74      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "# Generate predictions from training data for training scores\n",
    "y_train_pred = lr.predict(X_train)\n",
    "# Save baseline f1 training score for comparison to testing & tuned scores\n",
    "lr_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline LR classification report (training):\\n')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8599649f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basline LR classification report (testing):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76      1509\n",
      "           1       0.76      0.70      0.73      1447\n",
      "\n",
      "    accuracy                           0.74      2956\n",
      "   macro avg       0.75      0.74      0.74      2956\n",
      "weighted avg       0.75      0.74      0.74      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from testing data for testing scores\n",
    "y_test_pred = lr.predict(X_test)\n",
    "lr_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Basline LR classification report (testing):\\n')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010dd805",
   "metadata": {},
   "source": [
    "With a simple baseline logistic regression model, I see an f1 test score of 0.73. With similar scores for training and testing, we do not see evidence of overfit.\n",
    "\n",
    "Now, I will tune hyperparameters. The most straightforward method of hyperparameter tuning is grid search with cross-validation. We provide a set of values for any or all model parameters, and the GridSearchCV object systematically tests every possible combination of those given parameters (i.e., it steps through a multidimensional 'grid' of parameter values).\n",
    "\n",
    "The 'CV' in GridSearchCV stands for cross-validation, which is a useful method to stave off overfitting. GridSearchCV applies cross-validation at each step of the grid search. The training data is split into k \"folds\", or equally-sized chunks (the default value for k is 3 in the case of GridSearchCV, but it can be altered). A model is trained from the current set of parameters, and then the model is fit to k-1 folds, leaving the kth aside to act as a test set. A model score is produced and saved. Then, a new model is fit to a different set of k-1 folds (using the same current set of parameters from the grid search), using a different chunk as the test set, producing a second score. This process is performed a total of k times, each iteration using a different chunk as the holdout set, until we have k scores. Those scores are then averaged. This provides us with a score corresponding to a certain set of parameters, and we can be more confident in this score, because the cross-validation lowers the chances that our score was impacted too much by the incidental nature of our overall train/test split. We have, in a sense, simulated the train-test split k times, without ever exposing the model to the real test data.\n",
    "\n",
    "Here, I will only test a range of regularization parameters, along with a random state for the sake of reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4fd6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'random_state': 42}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100,],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr_grid = GridSearchCV(estimator=lr, param_grid=param_grid)\n",
    "start_time = time()\n",
    "lr_grid.fit(X_train, y_train)\n",
    "# Capture wall time for fitting the tuned model for later comparison\n",
    "lr_grid_fit_time = time() - start_time\n",
    "# Display best parameters from grid search\n",
    "lr_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d12675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LR classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76      3558\n",
      "           1       0.75      0.69      0.72      3338\n",
      "\n",
      "    accuracy                           0.74      6896\n",
      "   macro avg       0.74      0.74      0.74      6896\n",
      "weighted avg       0.74      0.74      0.74      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr_grid.predict(X_train)\n",
    "lr_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned LR classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "042bcff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LR classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.76      1509\n",
      "           1       0.76      0.70      0.73      1447\n",
      "\n",
      "    accuracy                           0.74      2956\n",
      "   macro avg       0.74      0.74      0.74      2956\n",
      "weighted avg       0.74      0.74      0.74      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = lr_grid.predict(X_test)\n",
    "# Capture wall time for making predictions for later comparison\n",
    "lr_grid_pred_time = time() - start_time\n",
    "lr_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned LR classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b26f37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in LR f1 score from parameter tuning: 0.001\n"
     ]
    }
   ],
   "source": [
    "print('Change in LR f1 score from parameter tuning:', round(lr_tuned_f1_test-lr_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001c642",
   "metadata": {},
   "source": [
    "There was barely any improvement, which is not entirely unexpected given the fact that I only tested a small number of values for one single parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4188c",
   "metadata": {},
   "source": [
    "### K-nearest neighbors\n",
    "\n",
    "Baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "114c432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline KNN classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84      3558\n",
      "           1       0.81      0.88      0.84      3338\n",
      "\n",
      "    accuracy                           0.84      6896\n",
      "   macro avg       0.84      0.84      0.84      6896\n",
      "weighted avg       0.84      0.84      0.84      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_train_pred = knn.predict(X_train)\n",
    "knn_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline KNN classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef99034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline KNN classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      1509\n",
      "           1       0.73      0.81      0.77      1447\n",
      "\n",
      "    accuracy                           0.76      2956\n",
      "   macro avg       0.77      0.77      0.76      2956\n",
      "weighted avg       0.77      0.76      0.76      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = knn.predict(X_test)\n",
    "knn_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline KNN classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf99a01",
   "metadata": {},
   "source": [
    "The baseline KNN model displayed some overfitting.\n",
    "\n",
    "Next, I will tune parameters by performing a grid search over a range of numbers of neighbors, as well as uniform vs. distance-based weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "993a6b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 9, 'weights': 'distance'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,3,5,7,9],\n",
    "    'weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=param_grid)\n",
    "start_time = time()\n",
    "knn_grid.fit(X_train, y_train)\n",
    "knn_grid_fit_time = time() - start_time\n",
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c941743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3558\n",
      "           1       1.00      1.00      1.00      3338\n",
      "\n",
      "    accuracy                           1.00      6896\n",
      "   macro avg       1.00      1.00      1.00      6896\n",
      "weighted avg       1.00      1.00      1.00      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = knn_grid.predict(X_train)\n",
    "knn_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned KNN classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94410e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.76      1509\n",
      "           1       0.74      0.82      0.78      1447\n",
      "\n",
      "    accuracy                           0.77      2956\n",
      "   macro avg       0.77      0.77      0.77      2956\n",
      "weighted avg       0.77      0.77      0.77      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = knn_grid.predict(X_test)\n",
    "knn_grid_pred_time = time() - start_time\n",
    "knn_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned KNN classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7e83041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.006\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(knn_tuned_f1_test-knn_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95756e",
   "metadata": {},
   "source": [
    "The model has memorized our training data, producing 100% training scores. With an f1 test score of 0.78, though, the tuned KNN performed a little better than the tuned LR model.\n",
    "\n",
    "Logistic regression and k-nearest neighbors are each single models, where classification is determined by a single calculation, and that calculation is shaped by the parameters the model learns from the training data. Such models can perform quite well under many circumstances. But we can also employ \"ensemble models\", which produce a large group of similarly-structured models, all of which get a say in the final classification decision. Ensemble models are shown to be more robust, because while a given individual model in the ensemble may produce a bias in a certain direction, another model will likely produce bias of a similar magnitude in the opposite direction. With enough members in the ensemble, the bias gets smoothed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b91bee",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forests are a popular ensemble model, where several decision trees are trained on bootstrapped samples from the training data. Classification is determined by a survey of decisions from the resulting \"forest\" of individual trees. In order to set a baseline, I will first create a random forest with all default parameters (setting the random state for reproducability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0808bb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RFC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3558\n",
      "           1       1.00      1.00      1.00      3338\n",
      "\n",
      "    accuracy                           1.00      6896\n",
      "   macro avg       1.00      1.00      1.00      6896\n",
      "weighted avg       1.00      1.00      1.00      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_train_pred = rfc.predict(X_train)\n",
    "rfc_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline RFC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b0ca4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RFC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80      1509\n",
      "           1       0.79      0.81      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = rfc.predict(X_test)\n",
    "rfc_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline RFC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4564b914",
   "metadata": {},
   "source": [
    "While we still see some overfitting to the training data, even the out-of-the-box random forest performs better on the test data than the tuned versions of our previous model candidates. Let's see what happens after tuning.\n",
    "\n",
    "When building a model with relatively few important parameters, and/or when you have reason to only test a small number of values for your parameters, GridSearchCV is feasible. However, as the number of parameters and values-per-parameter increase, grid searching quickly becomes infeasible in terms of computing capacity and time. This is when we turn to a related method: RandomizedSearchCV. Rather than testing every possible combination of parameter values, RandomSearchCV selects a specified number of combinations, chosen at random, which drastically reduces the number of models evaluated. And yet this process has been shown to perform at least nearly as well as the brute-force grid search. This is likely due to the fact that only a subset of parameters are likely to produce much difference in a model's performance on any given dataset. To test every possible combination would mean producing a lot of practically redundant models. The randomized search does a pretty good job capturing, or approximating, the actual variation in performance for the provided parameter grid.\n",
    "\n",
    "Since the RandomForestClassifier has so many potentially impactful parameters, I will employ RandomSearchCV. Guidance for this section came from sklearn documentation and this blog post: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74. Since there are about 1,000 parameter combinations in my grid, I will test `n_iter = 100` combinations, or 10% of the grid space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9803ad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 800,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': None,\n",
       " 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100,500,800,1000,1500,2000],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': [20,50,70,100,None],\n",
    "    'max_features': ['auto','sqrt'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'random_state':[42]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_random = RandomizedSearchCV(\n",
    "    estimator=rfc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "rfc_random.fit(X_train, y_train)\n",
    "rfc_random_fit_time = time() - start_time\n",
    "rfc_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc0611",
   "metadata": {},
   "source": [
    "The random search process took considerable time. I would not even attempt a full GridSearchCV with the above parameter grid. Let's see how the tuned model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c938575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned RFC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      3558\n",
      "           1       0.94      0.96      0.95      3338\n",
      "\n",
      "    accuracy                           0.95      6896\n",
      "   macro avg       0.95      0.95      0.95      6896\n",
      "weighted avg       0.95      0.95      0.95      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = rfc_random.predict(X_train)\n",
    "rfc_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned RFC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f8b7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned RFC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.80      1509\n",
      "           1       0.79      0.83      0.81      1447\n",
      "\n",
      "    accuracy                           0.81      2956\n",
      "   macro avg       0.81      0.81      0.81      2956\n",
      "weighted avg       0.81      0.81      0.81      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = rfc_random.predict(X_test)\n",
    "rfc_random_pred_time = time() - start_time\n",
    "rfc_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned RFC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6117cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.009\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(rfc_tuned_f1_test-rfc_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2fef6a",
   "metadata": {},
   "source": [
    "Nearly a full percentage-point increase in the f1 score. That means that not only did the random forest model have the best baseline metrics, it also responded most drastically to our parameter tuning. We also see less overfitting in the tuned model, with the training scores around 95% instead of 100%. This is a good illustration of the power of ensemble models, drawing on the \"wisdom of crowds\" to generate classifications by surveying outputs from hundreds of models.\n",
    "\n",
    "But is this the best we can do?\n",
    "\n",
    "### GradientBoostingClassifier\n",
    "\n",
    "The process of training a random forest classifier involves creating any number of _independent_ decision trees. The gradient boosting classifier, also a tree-based ensemble model, involves training a number of trees _in sequence_, where each subsequent tree is fitted to the error of the previous tree. In this way, each tree learns more about the data, getting better in specific ways to compensate for past shortcomings. The classification is determined by the final \"smartest\" tree in the sequence. Let's see how a baseline GB model performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9db1a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GBC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82      3558\n",
      "           1       0.79      0.85      0.82      3338\n",
      "\n",
      "    accuracy                           0.82      6896\n",
      "   macro avg       0.82      0.82      0.82      6896\n",
      "weighted avg       0.82      0.82      0.82      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)\n",
    "y_train_pred = gbc.predict(X_train)\n",
    "gbc_baseline_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Baseline GBC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8602b984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline GBC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      1509\n",
      "           1       0.77      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = gbc.predict(X_test)\n",
    "gbc_baseline_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Baseline GBC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9337fb",
   "metadata": {},
   "source": [
    "The testing scores of the RFC and GBC were quite simliar. However, we see _much_ less overfitting in the GBC model. This is not surprising, since the GBC model is designed to reduce overfitting. Hopefully, we can improve performance with tuning, so that we have the sweet-spot of good testing scores and no overfitting.\n",
    "\n",
    "I will again employ the randomized search with cross-validation. This grid could produce about 5,000 parameter combinations, so I will use a higher `n_iter` value to still capture a good portion of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "328c47f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'random_state': 42,\n",
       " 'n_estimators': 200,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 5,\n",
       " 'loss': 'exponential',\n",
       " 'learning_rate': 0.1,\n",
       " 'criterion': 'mse'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'learning_rate': [0.01,0.1,1],\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'criterion': ['friedman_mse','mse'],\n",
    "    'min_samples_leaf': [1,2,4],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'max_depth': [2,3,4,5],\n",
    "    'random_state': [42],\n",
    "    'max_features': ['auto','sqrt','log2']\n",
    "}\n",
    "\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc_random = RandomizedSearchCV(\n",
    "    estimator=gbc,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=500,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "start_time = time()\n",
    "gbc_random.fit(X_train, y_train)\n",
    "gbc_random_fit_time = time() - start_time\n",
    "gbc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0deb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned GBC classification report (training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86      3558\n",
      "           1       0.83      0.90      0.87      3338\n",
      "\n",
      "    accuracy                           0.87      6896\n",
      "   macro avg       0.87      0.87      0.87      6896\n",
      "weighted avg       0.87      0.87      0.87      6896\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = gbc_random.predict(X_train)\n",
    "gbc_tuned_f1_train = f1_score(y_train, y_train_pred)\n",
    "print('Tuned GBC classification report (training):')\n",
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9698139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned GBC classification report (testing):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.76      0.79      1509\n",
      "           1       0.77      0.84      0.80      1447\n",
      "\n",
      "    accuracy                           0.80      2956\n",
      "   macro avg       0.80      0.80      0.80      2956\n",
      "weighted avg       0.80      0.80      0.80      2956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "y_test_pred = gbc_random.predict(X_test)\n",
    "gbc_random_pred_time = time() - start_time\n",
    "gbc_tuned_f1_test = f1_score(y_test, y_test_pred)\n",
    "print('Tuned GBC classification report (testing):')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0345a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in f1 score from parameter tuning: 0.003\n"
     ]
    }
   ],
   "source": [
    "print('Change in f1 score from parameter tuning:', round(gbc_tuned_f1_test-gbc_baseline_f1_test, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64afa561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8035714285714287, 0.8002645502645502)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_tuned_f1_test, gbc_baseline_f1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834bba9",
   "metadata": {},
   "source": [
    "So, the GBC was not improved as much by tuning as the RFC, but the GBC was much more resistant to overfitting to the training data.\n",
    "\n",
    "## Comparing model performance\n",
    "\n",
    "Throughout the previous section, I checked the effect of tuning on a key model metric. I chose the f1 score as the key metric, because it is a good balance (i.e., harmonic mean) of precision and recall scores. In this use case, there is no reason to think we need to give special priority to reducing either false-positive or false-negative scores, so we do not need to particularly optimize precision or recall.\n",
    "\n",
    "Now, I will organize f1 scores (basline and tuned, training and testing) for each model, as well as fit- and prediction-times, into a DataFrame for easy comparison and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38677109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_f1_train</th>\n",
       "      <th>baseline_f1_test</th>\n",
       "      <th>tuned_f1_train</th>\n",
       "      <th>tuned_f1_test</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>prediction_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogReg</th>\n",
       "      <td>0.719749</td>\n",
       "      <td>0.728058</td>\n",
       "      <td>0.720811</td>\n",
       "      <td>0.729159</td>\n",
       "      <td>1.165144</td>\n",
       "      <td>0.006431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.842621</td>\n",
       "      <td>0.771100</td>\n",
       "      <td>0.998350</td>\n",
       "      <td>0.777485</td>\n",
       "      <td>5.882018</td>\n",
       "      <td>0.242815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC</th>\n",
       "      <td>0.998353</td>\n",
       "      <td>0.797672</td>\n",
       "      <td>0.951620</td>\n",
       "      <td>0.806321</td>\n",
       "      <td>4584.786701</td>\n",
       "      <td>0.478541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.821274</td>\n",
       "      <td>0.800265</td>\n",
       "      <td>0.866244</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>1594.196677</td>\n",
       "      <td>0.065465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        baseline_f1_train  baseline_f1_test  tuned_f1_train  tuned_f1_test  \\\n",
       "LogReg           0.719749          0.728058        0.720811       0.729159   \n",
       "KNN              0.842621          0.771100        0.998350       0.777485   \n",
       "RFC              0.998353          0.797672        0.951620       0.806321   \n",
       "GBC              0.821274          0.800265        0.866244       0.803571   \n",
       "\n",
       "           fit_time  prediction_time  \n",
       "LogReg     1.165144         0.006431  \n",
       "KNN        5.882018         0.242815  \n",
       "RFC     4584.786701         0.478541  \n",
       "GBC     1594.196677         0.065465  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    {\n",
    "        'baseline_f1_train':[lr_baseline_f1_train, knn_baseline_f1_train, rfc_baseline_f1_train, gbc_baseline_f1_train],\n",
    "        'baseline_f1_test':[lr_baseline_f1_test, knn_baseline_f1_test, rfc_baseline_f1_test, gbc_baseline_f1_test],\n",
    "        'tuned_f1_train':[lr_tuned_f1_train, knn_tuned_f1_train, rfc_tuned_f1_train, gbc_tuned_f1_train],\n",
    "        'tuned_f1_test':[lr_tuned_f1_test, knn_tuned_f1_test, rfc_tuned_f1_test, gbc_tuned_f1_test],\n",
    "        'fit_time':[lr_grid_fit_time, knn_grid_fit_time, rfc_random_fit_time, gbc_random_fit_time],\n",
    "        'prediction_time':[lr_grid_pred_time, knn_grid_pred_time, rfc_random_pred_time, gbc_random_pred_time]\n",
    "    },\n",
    "    index=['LogReg','KNN','RFC','GBC']\n",
    ")\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e03205",
   "metadata": {},
   "source": [
    "In every case, parameter-tuning yielded some improvement in f1, both in terms of training and test scores. Although the tuned RFC showed the highest test f1 score (by an ever-so-slight margin), we see a greater degree of over-fitting via the training score as compared to the GBC. Furthermore, the GBC required about 1/3 of the wall-time to fit the data, and about 1/9 of the time to generate predictions.\n",
    "\n",
    "## Conclusions and next steps\n",
    "\n",
    "After collecting ten thousand tracks from Spotify's API, performing exploratory data analysis, cleaning and preprocessing the data, and eliminating what appeared to be irrelevant/redundant features, I ended up with a set of 9,852 tracks, each containing the following features:\n",
    "\n",
    "- `single` - (Bool) Standalone single or part of an album/compilation\n",
    "- `danceability`, `energy`, `instrumentalness` - (Float) Spotify-provided measurements of musical qualities, scaled 0 to 1\n",
    "- `explicit` - (Bool) Includes explicit lyrics\n",
    "- `collab` - (Bool) Includes one or more guest artists\n",
    "- `timesig` - (Category) Time signature; a rhythmic measurement\n",
    "- `duration_s` - (Float) Length in seconds, scaled 0 to 1\n",
    "- __`popularity`__ (Bool) Target\n",
    "\n",
    "I tried two different classification models: Logistic regression and k-nearest neighbors. Then, I tried two ensemble classification models: Random forest and gradient boosting. For all four model types, I applied some hyperparameter-tuning in an attempt to optimize performance.\n",
    "\n",
    "In addition to viewing classification reports for all models, I specifically recorded the train f1 and test f1 scores, singling these out as the primary metrics for comparison. I chose the f1 score due to its nature as a harmonic mean of precision and recall; there is no apparent reason to optimize either individually in this use-case.  I also recorded wall-times for the fitting and predicting steps of each model, for a gauge of practicality in deployment.\n",
    "\n",
    "The random forest classifier yielded a test f1 score of 80.6%, to the gradient boosting classifier's 80.3% This 0.03% difference is not trivial, especially given the big-data context of Spotify's library and user base. However, the RFC's train f1 score is 95.1% to the GBC's 86.7%, indicating much more over-fitting in the RFC. This means that we can have somewhat less confidence in the RFC's ability to generalize to new data (beyond the test set), especially with such a slim margin between the test f1 scores. Moreover, the fitting and predicting times for the RFC were drastically higher than the GBC, and that is an important practical consideration when deploying an ML model.\n",
    "\n",
    "__Overall, I judge the tuned gradient boosting classifier to be the superior model among these candidates__. With further time, one could dig deeper into tuning the random forest, perhaps with Bayesian optimization, in the hopes of bringing that test f1 score closer to the train f1 score. If the RFC's f1 score could be improved sufficiently higher than that of the GBC, that could outweigh its lengthy fit and predict times and make it the superior model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa2750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
